# Copyright (c) 2023, NVIDIA CORPORATION.  All rights reserved.

"""Enhanced Storage writer for PyT Distributed format allowing asynchronous save with pipeline support."""

import dataclasses
import inspect
import logging
import os
import pickle
import queue
import threading
import math
from functools import partial
from heapq import heappop, heappush
from itertools import chain
from operator import itemgetter
from pathlib import Path
from time import time
from typing import Any, Callable, Dict, List, Optional, Tuple, Union

import torch
from torch import multiprocessing as mp
from torch.distributed.checkpoint import FileSystemWriter
from torch.distributed.checkpoint.filesystem import DEFAULT_SUFFIX, _StoragePrefix, _write_item
from torch.distributed.checkpoint.metadata import Metadata

try:
    from torch.distributed.checkpoint.filesystem import _StorageWriterTransforms
except ImportError:
    _StorageWriterTransforms = Any

from torch.distributed.checkpoint.planner import SavePlan, SavePlanner, WriteItem, WriteItemType
from torch.distributed.checkpoint.storage import WriteResult
from torch.futures import Future

from .async_utils import _disable_gc

logger = logging.getLogger(__name__)

WriteBucket = Tuple[Path, str, Tuple[list, list]]  # represents writes to a single file

# Type definitions for grouped pipeline functionality
TensorGroup = List[WriteBucket]
GroupedWriteBuckets = List[TensorGroup]

try:
    import psutil

    HAVE_PSUTIL = True
except ImportError:
    HAVE_PSUTIL = False

_results_queue = None


def _get_write_results_queue():
    global _results_queue
    if _results_queue is None:
        _results_queue = mp.Queue()
    return _results_queue


def _process_memory():
    """Get current process memory usage in MB."""
    if HAVE_PSUTIL:
        return psutil.Process().memory_info().rss / 1024 / 1024
    return 0


class PipelineStage:
    """Represents a pipeline stage for checkpoint data transfer with correct sequencing."""
    
    def __init__(self, stage_id: int, group_data: TensorGroup):
        self.stage_id = stage_id
        self.group_data = group_data
        self.gpu_to_cpu_done = threading.Event()
        self.cpu_to_disk_done = threading.Event()
        self.preloaded_data: Optional[List[WriteBucket]] = None
        self.error: Optional[Exception] = None


class FileSystemWriterAsync(FileSystemWriter):
    """
    Enhanced Async-enabled implementation of FileSystemWriter using file I/O with pipeline support.

    This class does not spawn the async process itself but relies on an external async mechanism.

    **Flow:**

    1. Call `write_data`
    2. Externally start an async process with `get_save_function_and_args` and its arguments.
    3. The async function `writer_proxy_func` calls `write_preloaded_data` across multiple
        processes.
    4. Once saving is finalized on all ranks, call `super().finish` with the results stored
        in `self.writer_result`.

    **Note:** Step (3) can also be executed synchronously.

    Currently, it is assumed that a separate writer is created for each ckpt save
    (intermediate state is stored as writer attributes).
    
    **Pipeline Support:**
    
    When enable_pipeline=True, this class supports:
    1. Grouping checkpoint parameters into multiple tensor groups
    2. Sequential GPU to CPU transfer between groups (Stage N waits for Stage N-1)
    3. Pipeline CPU->disk write of Stage N overlaps with GPU->CPU transfer of Stage N+1
    4. CPU->disk write of Stage N waits for CPU->disk write of Stage N-1 to complete
    """

    def __init__(
        self,
        path: Union[str, os.PathLike],
        *args,
        separation_hint: Optional[str] = None,
        use_msc: bool = False,
        enable_pipeline: bool = False,
        num_tensor_groups: int = 2,
        **kwargs,
    ):
        self.checkpoint_dir = path
        self.use_msc = use_msc
        self.enable_pipeline = enable_pipeline
        self.num_tensor_groups = max(2, num_tensor_groups) if enable_pipeline else 2

        super().__init__(path, *args, **kwargs)
        if not self.single_file_per_rank:
            raise NotImplementedError(
                "single_file_per_rank flag not supported for FileSystemWriterAsync"
            )

        self.can_run_decentralized_global_plan: bool = True

        # Intermediate state between preparation and finalization
        self.write_buckets: Optional[List[WriteBucket]] = None
        self.results_queue: Optional[mp.Queue] = None
        self.separation_hint = separation_hint

    def prepare_write_data(self, plan: SavePlan, planner: SavePlanner) -> None:
        """
        First stage of async saving. Copy data to CPU and plan the local saving.

        Args:
            plan (SavePlan): save plan generated by the PyT Distributed compatible planner
            planner (SavePlanner): save planner used to resolve the bytes and tensor data

        Returns: None, but stores the save plan in `self.write_buckets`
        """
        storage_plan: _StoragePrefix = plan.storage_data
        start = time()
        logger.debug(f"thread_count: {self.thread_count}, time: {start}")
        if self.separation_hint:
            assert (
                self.thread_count > 1
            ), "thread_count must be at least 2 if separation_hint is provided"
        bins = self.thread_count // 2 if self.separation_hint is not None else self.thread_count
        item_buckets = _split_by_size_and_type(bins, plan.items)
        logger.debug(f"bucket_prep, time: {time() - start}")

        start = time()
        # move tensors from GPU to CPU before starting async writing
        # We do D2H synchronously for now
        file_count = 0

        def gen_file(prefix=""):
            nonlocal file_count
            file_name = f"{prefix}{storage_plan.prefix}{file_count}{DEFAULT_SUFFIX}"
            file_count += 1
            return file_name

        def _clone_if_needed(ten: torch.Tensor):
            """Clone if we detect incontiguous storage for CPU tensors

            Makes sure we perform a `clone` only if we detect incontiguous storage,
            so that we don't blow up host memory unnecessarily.

            TODO: For persistent worker, this work should be changed to move the cpu tensor
            to shared_memory.
            """
            ten = ten.detach()
            if ten.device.type != "cpu":
                # We do D2H later when the async_request is scheduled for both sync / async
                # checkpointing
                return ten
            is_view = ten.untyped_storage().size() != ten.numel() * ten.itemsize
            return ten.clone() if is_view else ten

        def _create_write_bucket(items: List[WriteItem]) -> WriteBucket:
            bytes_data = []
            tensor_data = []
            file_name = gen_file()

            for item in items:
                if item.type == WriteItemType.BYTE_IO:
                    bytes_data.append((item.fqn, planner.resolve_data(item)))
                else:
                    tensor_data.append((item.fqn, _clone_if_needed(planner.resolve_tensor(item))))

            return (Path(file_name), storage_plan.prefix, (bytes_data, tensor_data))

        self.write_buckets = [_create_write_bucket(bucket) for bucket in item_buckets]
        logger.debug(f"write_buckets_prep, time: {time() - start}")

    def get_save_function_and_args(self) -> Tuple[Optional[Callable], Optional[Callable], List]:
        """
        Enhanced version that returns pipeline or standard functions based on enable_pipeline flag.
        
        Returns: None (if nothing to write) or tuple of:
            1) the function that saves the data (pipeline or standard)
            2) the function that stages GPU tensors (pipeline or standard) 
            3) arguments to the save function
        """
        if not self.write_buckets:
            return None, None, []
            
        transform_list = [self.transforms] if hasattr(self, "transforms") else []
        
        if self.enable_pipeline:
            # Return pipeline-enabled functions
            return (
                partial(self.write_preloaded_data_multiproc_pipeline, transform_list, self.use_msc),
                partial(self.preload_tensors_pipeline, self.write_buckets, True, self.num_tensor_groups),
                [torch.distributed.get_rank(), self.write_buckets, self.results_queue],
            )
        else:
            # Return standard functions (existing behavior)
            return (
                partial(self.write_preloaded_data_multiproc, transform_list, self.use_msc),
                partial(self.preload_tensors, self.write_buckets, True),
                [torch.distributed.get_rank(), self.write_buckets, self.results_queue],
            )

    @staticmethod
    def group_write_buckets_by_size(write_buckets: List[WriteBucket], num_groups: int) -> GroupedWriteBuckets:
        """
        Group write buckets into multiple groups based on tensor sizes for optimal pipeline processing.
        
        Args:
            write_buckets: List of WriteBucket objects to group
            num_groups: Number of groups to create
            
        Returns:
            List of tensor groups, each containing a subset of write buckets
        """
        if num_groups <= 1:
            return [write_buckets]
            
        if len(write_buckets) <= num_groups:
            # If we have fewer buckets than groups, put each bucket in its own group
            return [[bucket] for bucket in write_buckets]
        
        # Calculate total tensor size for each bucket
        bucket_sizes = []
        for bucket in write_buckets:
            file_name, storage_key, (bytes_data, tensor_data) = bucket
            total_size = 0
            
            # Calculate size of tensor data
            for item, tensor in tensor_data:
                if hasattr(tensor, 'numel') and hasattr(tensor, 'element_size'):
                    total_size += tensor.numel() * tensor.element_size()
                elif hasattr(tensor, 'nbytes'):
                    total_size += tensor.nbytes
                    
            bucket_sizes.append((bucket, total_size))
        
        # Sort buckets by size (largest first) for better load balancing
        bucket_sizes.sort(key=lambda x: x[1], reverse=True)
        
        # Distribute buckets across groups using a greedy algorithm
        groups = [[] for _ in range(num_groups)]
        group_sizes = [0] * num_groups
        
        for bucket, size in bucket_sizes:
            # Find the group with minimum current size
            min_group_idx = min(range(num_groups), key=lambda i: group_sizes[i])
            groups[min_group_idx].append(bucket)
            group_sizes[min_group_idx] += size
            
        # Filter out empty groups
        return [group for group in groups if group]

    @staticmethod
    def preload_tensors_pipeline(
        write_buckets: List[WriteBucket], 
        non_blocking: bool = True, 
        num_groups: int = 2
    ) -> List[WriteBucket]:
        """
        Pipeline-enabled tensor preloading with grouped sequential GPU->CPU transfer.
        
        This function groups tensors and performs sequential GPU->CPU transfer within each group,
        which is then used in the pipeline with CPU->disk writing.
        
        Args:
            write_buckets: List of WriteBucket objects
            non_blocking: Whether to use non-blocking GPU->CPU transfer
            num_groups: Number of groups to split tensors into
            
        Returns:
            List of WriteBucket objects with tensors moved to CPU
        """
        if not write_buckets:
            return []
            
        # Group write buckets for pipeline processing
        grouped_buckets = FileSystemWriterAsync.group_write_buckets_by_size(write_buckets, num_groups)
        
        logger.info(f"Pipeline preload: Processing {len(write_buckets)} buckets in {len(grouped_buckets)} groups")
        
        result = []
        
        # Process each group sequentially (this is the key constraint)
        for group_idx, group_buckets in enumerate(grouped_buckets):
            logger.debug(f"Processing group {group_idx + 1}/{len(grouped_buckets)} with {len(group_buckets)} buckets")
            
            # Process all buckets in this group
            for bucket in group_buckets:
                file_name, storage_key, (bytes_data, tensor_data) = bucket
                tensor_data = [
                    (item, tensor.to("cpu", non_blocking=non_blocking)) for item, tensor in tensor_data
                ]
                result.append((file_name, storage_key, (bytes_data, tensor_data)))
            
            # Synchronize after each group to ensure proper sequencing
            if non_blocking:
                torch.cuda.synchronize()
                
        logger.info(f"Pipeline preload completed: {len(result)} buckets processed")
        return result

    @staticmethod
    def preload_tensors(write_buckets: List[WriteBucket], non_blocking=True) -> List[WriteBucket]:
        """
        Standard tensor preloading (original implementation).

        Args:
            write_buckets (List): List of `WriteBucket` objects that define what to
                save in a checkpoint.
            non_blocking (bool, optional): knob to enable pinned D2H memcpy. Default is True.
        """
        result = []

        for bucket in write_buckets:
            file_name, storage_key, (bytes_data, tensor_data) = bucket
            tensor_data = [
                (item, tensor.to("cpu", non_blocking=non_blocking)) for item, tensor in tensor_data
            ]
            result.append((file_name, storage_key, (bytes_data, tensor_data)))
        if non_blocking:
            torch.cuda.synchronize()
        return result

    @staticmethod
    @_disable_gc()
    def write_preloaded_data_multiproc_pipeline(
        transform_list: List[_StorageWriterTransforms],
        use_msc: bool,
        rank: int,
        write_buckets: List[WriteBucket],
        global_results_queue: mp.Queue,
    ) -> None:
        """
        Pipeline-enabled multiprocess data writing with correct stage sequencing.
        
        This function implements the corrected pipeline logic where:
        1. GPU->CPU transfers are sequential across stages
        2. CPU->disk writes are sequential across stages  
        3. Stage N's CPU->disk write overlaps with Stage N+1's GPU->CPU transfer
        
        Args:
            transform_list: List of storage writer transforms
            use_msc: Whether to use MSC (Multi-Storage Client)
            rank: Current process rank
            write_buckets: List of WriteBucket objects to process
            global_results_queue: Queue for collecting results
        """
        if not write_buckets:
            return
            
        # Group write buckets for pipeline processing
        grouped_buckets = FileSystemWriterAsync.group_write_buckets_by_size(write_buckets, 2)  # Use 2 groups for pipeline
        
        logger.info(f"Pipeline multiproc: Processing {len(write_buckets)} buckets in {len(grouped_buckets)} groups")
        
        # Create pipeline stages
        stages = []
        for stage_id, group_buckets in enumerate(grouped_buckets):
            stage = PipelineStage(stage_id, group_buckets)
            stages.append(stage)
        
        # Create orchestrator for pipeline execution
        orchestrator = _PipelineOrchestrator(stages, transform_list, use_msc, rank, global_results_queue)
        
        # Start pipeline execution
        orchestrator.execute_pipeline()

    @staticmethod
    @_disable_gc()
    def write_preloaded_data_multiproc(
        transform_list: List[_StorageWriterTransforms],
        use_msc: bool,
        rank: int,
        write_buckets: List[WriteBucket],
        global_results_queue: mp.Queue,
    ) -> None:
        """
        Performs saving data to storage with multiple processes (original implementation).

        Starts predefined number of processes and uses 2 queues to make sure the results
        are complete:
        - local_results_queue - to send the actual results
        - count_queue - small queue to mark worker as completed

        Using just one queue disallowed proper exception handling.

        Args:
            transform_list: List of storage writer transforms
            use_msc: Whether to use MSC (Multi-Storage Client)
            rank: Current process rank
            write_buckets: List of WriteBucket objects to process
            global_results_queue: Queue for collecting results
        """
        if not write_buckets:
            return

        local_results_queue = mp.Queue()
        count_queue = mp.Queue()

        processes = []
        for bucket in write_buckets:
            p = mp.Process(
                target=FileSystemWriterAsync.write_preloaded_data,
                args=(transform_list, use_msc, bucket, local_results_queue, count_queue),
            )
            p.start()
            processes.append(p)

        # Collect results
        completed = 0
        total = len(write_buckets)
        while completed < total:
            try:
                result = local_results_queue.get(timeout=1)
                global_results_queue.put(result)
                completed += 1
            except queue.Empty:
                # Check if any process has died
                for p in processes:
                    if not p.is_alive():
                        p.join()
                        raise RuntimeError(f"Process {p.pid} died unexpectedly")

        # Wait for all processes to complete
        for p in processes:
            p.join()

    @staticmethod
    def write_preloaded_data(
        transform_list: List[_StorageWriterTransforms],
        use_msc: bool,
        bucket: WriteBucket,
        local_results_queue: mp.Queue,
        count_queue: mp.Queue,
    ) -> None:
        """
        Writes preloaded data for a single bucket (original implementation).

        Args:
            transform_list: List of storage writer transforms
            use_msc: Whether to use MSC (Multi-Storage Client)
            bucket: WriteBucket to process
            local_results_queue: Queue for results
            count_queue: Queue for completion count
        """
        try:
            file_name, storage_key, (bytes_data, tensor_data) = bucket

            # Apply transforms
            for transform in transform_list:
                bytes_data, tensor_data = transform(bytes_data, tensor_data)

            # Write the data
            if use_msc:
                from megatron.core.msc_utils import MultiStorageClientFeature

                with MultiStorageClientFeature.open_file(file_name, "wb") as f:
                    _write_item(f, bytes_data, tensor_data)
            else:
                with open(file_name, "wb") as f:
                    _write_item(f, bytes_data, tensor_data)

            # Put result in queue
            local_results_queue.put(WriteResult(index=storage_key, size=0))
            count_queue.put(1)

        except Exception as e:
            logger.error(f"Error writing bucket {bucket[0]}: {e}")
            local_results_queue.put(WriteResult(index=bucket[1], size=0, exception=e))
            count_queue.put(1)


class _PipelineOrchestrator:
    """Orchestrates the pipeline execution with correct stage sequencing."""
    
    def __init__(self, stages: List[PipelineStage], transform_list: List[_StorageWriterTransforms], 
                 use_msc: bool, rank: int, global_results_queue: mp.Queue):
        self.stages = stages
        self.transform_list = transform_list
        self.use_msc = use_msc
        self.rank = rank
        self.global_results_queue = global_results_queue
        
    def execute_pipeline(self):
        """Execute the pipeline with correct stage sequencing."""
        # Start GPU->CPU transfer for all stages (sequential)
        gpu_to_cpu_threads = []
        for stage in self.stages:
            thread = threading.Thread(target=self.gpu_to_cpu_worker, args=(stage,))
            thread.start()
            gpu_to_cpu_threads.append(thread)
        
        # Start CPU->disk write for all stages (sequential, with proper dependencies)
        cpu_to_disk_threads = []
        for stage in self.stages:
            thread = threading.Thread(target=self.cpu_to_disk_worker, args=(stage,))
            thread.start()
            cpu_to_disk_threads.append(thread)
        
        # Wait for all threads to complete
        for thread in gpu_to_cpu_threads + cpu_to_disk_threads:
            thread.join()
    
    def gpu_to_cpu_worker(self, stage: PipelineStage):
        """Worker for GPU->CPU transfer with sequential dependencies."""
        try:
            # CRITICAL: Wait for previous stage's GPU->CPU to complete
            if stage.stage_id > 0:
                prev_stage = self.stages[stage.stage_id - 1]
                prev_stage.gpu_to_cpu_done.wait()
            
            logger.debug(f"Stage {stage.stage_id}: Starting GPU->CPU transfer for {len(stage.group_data)} buckets")
            
            # Perform GPU->CPU transfer for this stage
            stage.preloaded_data = []
            for bucket in stage.group_data:
                file_name, storage_key, (bytes_data, tensor_data) = bucket
                tensor_data = [
                    (item, tensor.to("cpu", non_blocking=True)) for item, tensor in tensor_data
                ]
                stage.preloaded_data.append((file_name, storage_key, (bytes_data, tensor_data)))
            
            # Synchronize after this stage
            torch.cuda.synchronize()
            
            # Signal completion
            stage.gpu_to_cpu_done.set()
            logger.debug(f"Stage {stage.stage_id}: GPU->CPU transfer completed")
            
        except Exception as e:
            logger.error(f"Stage {stage.stage_id}: GPU->CPU transfer failed: {e}")
            stage.error = e
            stage.gpu_to_cpu_done.set()
    
    def cpu_to_disk_worker(self, stage: PipelineStage):
        """Worker for CPU->disk write with sequential dependencies."""
        try:
            # 1. Wait for current stage's GPU->CPU transfer to complete
            stage.gpu_to_cpu_done.wait()
            
            # 2. CRITICAL: Wait for previous stage's CPU->disk write to complete
            if stage.stage_id > 0:
                prev_stage = self.stages[stage.stage_id - 1]
                prev_stage.cpu_to_disk_done.wait()
            
            logger.debug(f"Stage {stage.stage_id}: Starting CPU->disk write for {len(stage.preloaded_data)} buckets")
            
            # Perform CPU->disk write for this stage
            if stage.preloaded_data:
                local_results_queue = mp.Queue()
                count_queue = mp.Queue()
                
                processes = []
                for bucket in stage.preloaded_data:
                    p = mp.Process(
                        target=FileSystemWriterAsync.write_preloaded_data,
                        args=(self.transform_list, self.use_msc, bucket, local_results_queue, count_queue),
                    )
                    p.start()
                    processes.append(p)
                
                # Collect results
                completed = 0
                total = len(stage.preloaded_data)
                while completed < total:
                    try:
                        result = local_results_queue.get(timeout=1)
                        self.global_results_queue.put(result)
                        completed += 1
                    except queue.Empty:
                        # Check if any process has died
                        for p in processes:
                            if not p.is_alive():
                                p.join()
                                raise RuntimeError(f"Process {p.pid} died unexpectedly")
                
                # Wait for all processes to complete
                for p in processes:
                    p.join()
            
            # Signal completion
            stage.cpu_to_disk_done.set()
            logger.debug(f"Stage {stage.stage_id}: CPU->disk write completed")
            
        except Exception as e:
            logger.error(f"Stage {stage.stage_id}: CPU->disk write failed: {e}")
            stage.error = e
            stage.cpu_to_disk_done.set()


def _split_by_size_and_type(bins: int, items: List[WriteItem]) -> List[List[WriteItem]]:
    """Split items into bins based on size and type for load balancing."""
    if bins <= 1:
        return [items]
    
    # Calculate size for each item
    item_sizes = []
    for item in items:
        if item.type == WriteItemType.BYTE_IO:
            size = item.storage_data.bytes_data_size
        else:
            size = item.storage_data.tensor_data_size
        item_sizes.append((item, size))
    
    # Sort by size (largest first)
    item_sizes.sort(key=lambda x: x[1], reverse=True)
    
    # Distribute across bins using greedy algorithm
    bins_list = [[] for _ in range(bins)]
    bin_sizes = [0] * bins
    
    for item, size in item_sizes:
        # Find bin with minimum current size
        min_bin_idx = min(range(bins), key=lambda i: bin_sizes[i])
        bins_list[min_bin_idx].append(item)
        bin_sizes[min_bin_idx] += size
    
    return bins_list
