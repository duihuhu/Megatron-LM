#!/usr/bin/env python3

"""
æµ‹è¯•æµæ°´çº¿checkpointé›†æˆåŠŸèƒ½
"""

import os
import sys
import torch
import tempfile
import shutil
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, '/Users/gaofz/Desktop/èƒ¡cunchen/Phd/LLM/ECTrain/Megatron-LM')

def test_filesystem_writer_async_pipeline():
    """æµ‹è¯• FileSystemWriterAsync çš„æµæ°´çº¿åŠŸèƒ½"""
    print("=== æµ‹è¯• FileSystemWriterAsync æµæ°´çº¿åŠŸèƒ½ ===")
    
    from megatron.core.dist_checkpointing.strategies.filesystem_async import FileSystemWriterAsync
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•
    temp_dir = tempfile.mkdtemp()
    try:
        # æµ‹è¯•æ ‡å‡†æ¨¡å¼
        print("1. æµ‹è¯•æ ‡å‡†æ¨¡å¼...")
        writer_std = FileSystemWriterAsync(
            temp_dir,
            enable_pipeline=False,
            num_tensor_groups=2
        )
        print(f"   æ ‡å‡†æ¨¡å¼ - enable_pipeline: {writer_std.enable_pipeline}")
        print(f"   æ ‡å‡†æ¨¡å¼ - num_tensor_groups: {writer_std.num_tensor_groups}")
        
        # æµ‹è¯•æµæ°´çº¿æ¨¡å¼
        print("2. æµ‹è¯•æµæ°´çº¿æ¨¡å¼...")
        writer_pipeline = FileSystemWriterAsync(
            temp_dir,
            enable_pipeline=True,
            num_tensor_groups=4
        )
        print(f"   æµæ°´çº¿æ¨¡å¼ - enable_pipeline: {writer_pipeline.enable_pipeline}")
        print(f"   æµæ°´çº¿æ¨¡å¼ - num_tensor_groups: {writer_pipeline.num_tensor_groups}")
        
        # æµ‹è¯• get_save_function_and_args
        print("3. æµ‹è¯• get_save_function_and_args...")
        
        # æ¨¡æ‹Ÿ write_buckets
        mock_buckets = [
            ("file1.pt", "tensor1", ([], [("item1", torch.randn(10, 10))])),
            ("file2.pt", "tensor2", ([], [("item2", torch.randn(20, 20))])),
        ]
        writer_std.write_buckets = mock_buckets
        writer_pipeline.write_buckets = mock_buckets
        
        # æ ‡å‡†æ¨¡å¼
        async_fn_std, preload_fn_std, args_std = writer_std.get_save_function_and_args()
        print(f"   æ ‡å‡†æ¨¡å¼ - async_fn: {async_fn_std.func.__name__ if hasattr(async_fn_std, 'func') else 'Unknown'}")
        print(f"   æ ‡å‡†æ¨¡å¼ - preload_fn: {preload_fn_std.func.__name__ if hasattr(preload_fn_std, 'func') else 'Unknown'}")
        
        # æµæ°´çº¿æ¨¡å¼
        async_fn_pipeline, preload_fn_pipeline, args_pipeline = writer_pipeline.get_save_function_and_args()
        print(f"   æµæ°´çº¿æ¨¡å¼ - async_fn: {async_fn_pipeline.func.__name__ if hasattr(async_fn_pipeline, 'func') else 'Unknown'}")
        print(f"   æµæ°´çº¿æ¨¡å¼ - preload_fn: {preload_fn_pipeline.func.__name__ if hasattr(preload_fn_pipeline, 'func') else 'Unknown'}")
        
        print("   âœ“ FileSystemWriterAsync æµæ°´çº¿åŠŸèƒ½æµ‹è¯•é€šè¿‡")
        
    finally:
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        shutil.rmtree(temp_dir, ignore_errors=True)

def test_torch_dist_save_strategy_pipeline():
    """æµ‹è¯• TorchDistSaveShardedStrategy çš„æµæ°´çº¿åŠŸèƒ½"""
    print("\n=== æµ‹è¯• TorchDistSaveShardedStrategy æµæ°´çº¿åŠŸèƒ½ ===")
    
    from megatron.core.dist_checkpointing.strategies.torch import TorchDistSaveShardedStrategy
    
    # æµ‹è¯•æ ‡å‡†æ¨¡å¼
    print("1. æµ‹è¯•æ ‡å‡†æ¨¡å¼...")
    strategy_std = TorchDistSaveShardedStrategy(
        'torch_dist', 1,
        enable_pipeline=False,
        num_tensor_groups=2
    )
    print(f"   æ ‡å‡†æ¨¡å¼ - enable_pipeline: {strategy_std.enable_pipeline}")
    print(f"   æ ‡å‡†æ¨¡å¼ - num_tensor_groups: {strategy_std.num_tensor_groups}")
    
    # æµ‹è¯•æµæ°´çº¿æ¨¡å¼
    print("2. æµ‹è¯•æµæ°´çº¿æ¨¡å¼...")
    strategy_pipeline = TorchDistSaveShardedStrategy(
        'torch_dist', 1,
        enable_pipeline=True,
        num_tensor_groups=6
    )
    print(f"   æµæ°´çº¿æ¨¡å¼ - enable_pipeline: {strategy_pipeline.enable_pipeline}")
    print(f"   æµæ°´çº¿æ¨¡å¼ - num_tensor_groups: {strategy_pipeline.num_tensor_groups}")
    
    print("   âœ“ TorchDistSaveShardedStrategy æµæ°´çº¿åŠŸèƒ½æµ‹è¯•é€šè¿‡")

def test_pipeline_parameters_validation():
    """æµ‹è¯•æµæ°´çº¿å‚æ•°éªŒè¯"""
    print("\n=== æµ‹è¯•æµæ°´çº¿å‚æ•°éªŒè¯ ===")
    
    from megatron.core.dist_checkpointing.strategies.filesystem_async import FileSystemWriterAsync
    
    # æµ‹è¯•å‚æ•°éªŒè¯
    print("1. æµ‹è¯•å‚æ•°éªŒè¯...")
    
    # æµ‹è¯• enable_pipeline=False æ—¶çš„ num_tensor_groups
    writer1 = FileSystemWriterAsync(
        "/tmp/test",
        enable_pipeline=False,
        num_tensor_groups=10  # åº”è¯¥è¢«å¿½ç•¥
    )
    print(f"   enable_pipeline=False, num_tensor_groups=10 -> {writer1.num_tensor_groups}")
    
    # æµ‹è¯• enable_pipeline=True æ—¶çš„ num_tensor_groups
    writer2 = FileSystemWriterAsync(
        "/tmp/test",
        enable_pipeline=True,
        num_tensor_groups=10
    )
    print(f"   enable_pipeline=True, num_tensor_groups=10 -> {writer2.num_tensor_groups}")
    
    # æµ‹è¯• num_tensor_groups < 2 çš„æƒ…å†µ
    writer3 = FileSystemWriterAsync(
        "/tmp/test",
        enable_pipeline=True,
        num_tensor_groups=1  # åº”è¯¥è¢«è°ƒæ•´ä¸º2
    )
    print(f"   enable_pipeline=True, num_tensor_groups=1 -> {writer3.num_tensor_groups}")
    
    print("   âœ“ æµæ°´çº¿å‚æ•°éªŒè¯æµ‹è¯•é€šè¿‡")

def test_group_write_buckets():
    """æµ‹è¯•åˆ†ç»„åŠŸèƒ½"""
    print("\n=== æµ‹è¯•åˆ†ç»„åŠŸèƒ½ ===")
    
    from megatron.core.dist_checkpointing.strategies.filesystem_async import FileSystemWriterAsync
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    write_buckets = [
        ("file1.pt", "tensor1", ([], [("item1", torch.randn(100, 100))])),
        ("file2.pt", "tensor2", ([], [("item2", torch.randn(200, 200))])),
        ("file3.pt", "tensor3", ([], [("item3", torch.randn(50, 50))])),
        ("file4.pt", "tensor4", ([], [("item4", torch.randn(300, 300))])),
    ]
    
    print(f"1. åŸå§‹ buckets æ•°é‡: {len(write_buckets)}")
    
    # æµ‹è¯•åˆ†ç»„
    grouped = FileSystemWriterAsync.group_write_buckets_by_size(write_buckets, 2)
    print(f"2. åˆ†æˆ 2 ç»„: {len(grouped)} ç»„")
    for i, group in enumerate(grouped):
        print(f"   ç»„ {i+1}: {len(group)} ä¸ª buckets")
    
    grouped = FileSystemWriterAsync.group_write_buckets_by_size(write_buckets, 3)
    print(f"3. åˆ†æˆ 3 ç»„: {len(grouped)} ç»„")
    for i, group in enumerate(grouped):
        print(f"   ç»„ {i+1}: {len(group)} ä¸ª buckets")
    
    print("   âœ“ åˆ†ç»„åŠŸèƒ½æµ‹è¯•é€šè¿‡")

def test_command_line_args():
    """æµ‹è¯•å‘½ä»¤è¡Œå‚æ•°"""
    print("\n=== æµ‹è¯•å‘½ä»¤è¡Œå‚æ•° ===")
    
    # æ¨¡æ‹Ÿå‘½ä»¤è¡Œå‚æ•°
    class MockArgs:
        def __init__(self):
            self.enable_pipeline_checkpoint = True
            self.num_tensor_groups = 4
            self.async_save = True
    
    args = MockArgs()
    
    print(f"1. æ¨¡æ‹Ÿå‘½ä»¤è¡Œå‚æ•°:")
    print(f"   --enable-pipeline-checkpoint: {args.enable_pipeline_checkpoint}")
    print(f"   --num-tensor-groups: {args.num_tensor_groups}")
    print(f"   --async-save: {args.async_save}")
    
    # æµ‹è¯•å‚æ•°è·å–
    enable_pipeline = getattr(args, 'enable_pipeline_checkpoint', False)
    num_tensor_groups = getattr(args, 'num_tensor_groups', 2)
    
    print(f"2. å‚æ•°è·å–ç»“æœ:")
    print(f"   enable_pipeline: {enable_pipeline}")
    print(f"   num_tensor_groups: {num_tensor_groups}")
    
    print("   âœ“ å‘½ä»¤è¡Œå‚æ•°æµ‹è¯•é€šè¿‡")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹æµ‹è¯•æµæ°´çº¿checkpointé›†æˆåŠŸèƒ½...\n")
    
    try:
        test_filesystem_writer_async_pipeline()
        test_torch_dist_save_strategy_pipeline()
        test_pipeline_parameters_validation()
        test_group_write_buckets()
        test_command_line_args()
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æµæ°´çº¿checkpointé›†æˆåŠŸèƒ½æ­£å¸¸å·¥ä½œã€‚")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())
